<!--Authors: Hui Ren (rhfeiyang.github.io)-->
<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Feature4X: Bridging Any Monocular Video to 4D Agentic AI with Versatile Gaussian Feature Fields">
  <meta name="keywords" content="Gaussian Splatting, Semantic Understanding, 4D reconstruction">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Feature4X: Bridging Any Monocular Video to 4D Agentic AI with Versatile Gaussian Feature Fields</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="data:,"> <!-- Empty favicon -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

<!-- slider scripts-->
  <script
          defer
          src="https://cdn.jsdelivr.net/npm/img-comparison-slider@8/dist/index.js"
  ></script>
  <link
          rel="stylesheet"
          href="https://cdn.jsdelivr.net/npm/img-comparison-slider@8/dist/styles.css"
  />

  <!-- swiper -->
  <!-- https://swiperjs.com/demos -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/swiper@10/swiper-bundle.min.css"/>
  <script src="https://cdn.jsdelivr.net/npm/swiper@10/swiper-bundle.min.js"></script>


  <!-- img comparison slider -->
  <!-- https://github.com/sneas/img-comparison-slider -->


</head>
<body>

<style>
  .container_teaser {
    display: flex; /* Enables Flexbox */
    justify-content: center; /* Horizontally centers the content */
    align-items: center; /* Vertically centers the content */
    height: 100%; /* Set the height of the container */
  }

  .column_teaser {
    flex: 0 0 auto; /* This ensures that the column doesn't grow or shrink */
    width: 100%; /* Adjust this width as needed */
    max-width: 500px; /* Adjust this max-width as needed */
    margin: 0 auto; /* Centers the column within the container */
  }
</style>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Feature4X: Bridging Any Monocular Video to 4D Agentic AI with Versatile Gaussian Feature Fields</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://shijiezhou-ucla.github.io/">Shijie Zhou*</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://rhfeiyang.github.io">Hui Ren*</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://yijiaweng.github.io/">Yijia Weng</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/shuwang-zhang">Shuwang Zhang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://zhenwangwz.github.io/">Zhen Wang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://ir1d.github.io/">Dejia Xu</a><sup>4</sup>,
            </span>
            <span class="author-block">
              <a href="https://zhiwenfan.github.io/">Zhiwen Fan</a><sup>4</sup>
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=LkpA-L0AAAAJ&hl=en">Suya You</a><sup>5</sup>
            </span>
            <span class="author-block">
              <a href="https://vita-group.github.io/">Zhangyang Wang</a><sup>4</sup>
            </span>
            <span class="author-block">
              <a href="https://profiles.stanford.edu/leonidas-guibas">Leonidas Guibas</a><sup>3</sup>
            </span>
            <span class="author-block">
              <a href="https://samueli.ucla.edu/people/achuta-kadambi/">Achuta Kadambi</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>UCLA,</span>
            <span class="author-block"><sup>2</sup>MIT,</span>
            <span class="author-block"><sup>3</sup>Stanford,</span>
            <span class="author-block"><sup>4</sup>UT-Austin,</span>
            <span class="author-block"><sup>5</sup>DEVCOM ARL</span>
          </div>
          <p class="is-size-6">*Co-first authors</p>
          <p class="is-size-5" style="color: rgb(169, 16, 151); --darkreader-inline-color: var(--darkreader-text-a91097, #ef5ade);">CVPR 2025</p>
          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="files/Feature4x.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!--              &lt;!&ndash; Video Link. &ndash;&gt;-->
              <!--              <span class="link-block">-->
              <!--                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"-->
              <!--                   class="external-link button is-normal is-rounded is-dark">-->
              <!--                  <span class="icon">-->
              <!--                      <i class="fab fa-youtube"></i>-->
              <!--                  </span>-->
              <!--                  <span>Video</span>-->
              <!--                </a>-->
              <!--              </span>-->
              <!-- Code Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!--              &lt;!&ndash; Dataset Link. &ndash;&gt;-->
              <!--              <span class="link-block">-->
              <!--                <a href="https://github.com/google/nerfies/releases/tag/0.1"-->
              <!--                   class="external-link button is-normal is-rounded is-dark">-->
              <!--                  <span class="icon">-->
              <!--                      <i class="far fa-images"></i>-->
              <!--                  </span>-->
              <!--                  <span>Data</span>-->
              <!--                  </a>-->
              <!--            </div>-->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video controls poster="./media/figs/fig1_feature4x.jpg" style="margin: 0 auto; max-width: 100%; width: auto; height: auto; display: block;">
        <source src="./media/demo.mp4" type="video/mp4">
        Demo Video
      </video>
      <h2 class="subtitle has-text-centered" style="font-size: 14px;">
        Feature4X: Building 4D Interactive Scenes with Agentic AI from Monocular Videos. By dynamically distilling modelconditioned features and integrating 2D foundation models with LLMs in feedback loops, Feature4X enables multimodal tasks across 2D, 3D, and 4D with high-level language inputs or direct user interactions, including (but not limited to) segmentation, scene editing, and VQA across novel views and all time steps, unlocking new possibilities for 4D agentic AI.
      </h2>
    </div>
  </div>
</section>

<section class="section hero is-light">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Recent advancements in 2D and multimodal models have achieved remarkable success by leveraging large-scale training on extensive datasets. However, extending these achievements to enable free-form interactions and high-level semantic operations with complex 3D/4D scenes remains challenging. This difficulty stems from the limited availability of large-scale, annotated 3D/4D or multi-view datasets, which are crucial for generalizable vision and language tasks such as open-vocabulary and prompt-based segmentation, language-guided editing, and visual question answering (VQA). In this paper, we introduce <em>Feature4X</em>, a universal framework designed to extend any functionality from 2D vision foundation model into the 4D realm, using only monocular video input, which is widely available from user-generated content. The "X" in Feature4X represents its versatility, enabling any task through adaptable, model-conditioned 4D feature field distillation. At the core of our framework is a dynamic optimization strategy that unifies multiple model capabilities into a single representation. Additionally, to the best of our knowledge, Feature4X is the first method to distill and lift the features of video foundation models (e.g. SAM2, InternVideo2) into an explicit <em>4D feature field</em> using Gaussian Splatting. Our experiments showcase novel view segment anything, geometric and appearance scene editing, and free-form VQA across all time steps, empowered by LLMs in feedback loops. These advancements broaden the scope of agentic AI applications by providing a foundation for scalable, contextually and spatiotemporally aware systems capable of immersive dynamic 4D scene interaction.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!--    &lt;!&ndash; Paper video. &ndash;&gt;-->
    <!--    <div class="columns is-centered has-text-centered">-->
    <!--      <div class="column is-four-fifths">-->
    <!--        <h2 class="title is-3">Video</h2>-->
    <!--        <div class="publication-video">-->
    <!--          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"-->
    <!--                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>-->
    <!--        </div>-->
    <!--      </div>-->
    <!--    </div>-->
    <!--    &lt;!&ndash;/ Paper video. &ndash;&gt;-->
  </div>
</section>

<br>
<section class="hero teaser">
  <div class="container is-max-desktop" >
    <!--    Method -->
    <div class="hero-body" style="text-align: center;">
      <h2 class="title is-3">Overview</h2>
      <!--      <h2 class="subtitle has-text-centered" style="font-size: 14px;">-->
      <p style="text-align: justify;">
        Given an input monocular video, we infer 2D priors to segment static background (represented by <em>static 3D Gaussians</em> augmented with latent features) and dynamic foreground (represented by <em>dynamic 3D Gaussians</em> guided by Motion Scaffolds, a set of nodes {v<sub>i</sub>} encoding 3D motion trajectories and latent features h<sub>1</sub>). Dynamic Gaussian features and motions are computed via interpolation from their K-nearest scaffold nodes. At each timestep, dynamic Gaussians are warped and fused with static Gaussians. A parallel rasterization generates RGB images and a unified latent feature map, decoded into task-specific features—illustrated here by SAM2, CLIP-LSeg, and InternVideo2 for representative 2D (novel view segmentation), 3D (scene editing), and 4D (spatiotemporal VQA) tasks. Our framework generalizes to any 2D vision foundation model and is trained end-to-end using input RGB frames and customized features from pretrained 2D models. At inference, rendered feature maps from arbitrary views and timesteps are directly fed into task-specific decoders, seamlessly supporting user prompts and LLM interactions to form a unified 4D agentic AI system.
      </p>
      <br>
      <img
              src="./media/figs/fig2_pipeline.jpg"
              alt="Method Pipeline"
              style="margin: 0 auto; max-width: 100%; width: auto; height: auto; display: block;"
      >

    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop" >
    <!--    Method -->
    <div class="hero-body" style="text-align: center;">
      <h2 class="title is-3">Segment Anything in Dynamic 4D Scenes with SAM2 Feature Field</h2>
      <!--      <h2 class="subtitle has-text-centered" style="font-size: 14px;">-->
      <p style="text-align: justify;">
        For any rendered novel view video, we support: (a) <em>Promptless segmentation</em> (segment everything): when no user prompt is provided, segmentation masks are automatically assigned at the first frame (t=0) and then propagated across all frames. (b) <em>Promptable segmentation</em> (segment anything): the user can segment any object—static or dynamic—at any timestep using a point or box prompt, and the corresponding mask is robustly tracked and propagated through subsequent frames.
      </p>
      <br>
      <img
              src="./media/figs/fig3_sam.png"
              alt="Segment Anything"
              style="margin: 0 auto; max-width: 100%; width: auto; height: auto; display: block;"
      >

    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop" >
    <!--    Method -->
    <div class="hero-body" style="text-align: center;">
      <h2 class="title is-3">Baseline Comparison on SAM2 Inference.</h2>
      <!--      <h2 class="subtitle has-text-centered" style="font-size: 14px;">-->
      <p style="text-align: justify;">
      We compare segmentation quality and inference speed between (a) the naive RGB-based approach and (b) our feature-based method. Ours achieves comparable segmentation, accurately tracking the object over time, and avoids RGB artifacts (red box region at t=70), while reducing inference time to about 4× speed-up.
      </p>
      <br>
      <img
              src="./media/figs/fig4_sam_compare.png"
              alt="Baseline Comparison"
              style="margin: 0 auto; max-width: 100%; width: auto; height: auto; display: block;"
      >

    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop" >
    <!--    Method -->
    <div class="hero-body" style="text-align: center;">
      <h2 class="title is-3">Semantic 4D Scene Understanding with CLIP Feature Field</h2>
      <!--      <h2 class="subtitle has-text-centered" style="font-size: 14px;">-->
      <p style="text-align: justify;">
        By lifting CLIP-LSeg features into a 4D feature field, we enable <em>pixel-level semantic segmentation</em> from any view at any timestep. This allows robust 4D scene understanding, even as object appearances change over time—for example, accurately identifying a blooming flower from bud to full bloom across views.
      </p>
      <br>
      <img
              src="./media/figs/fig5_clip_seg_results.jpg"
              alt="Scene Understanding"
              style="margin: 0 auto; max-width: 100%; width: auto; height: auto; display: block;"
      >

    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop" >
    <!--    Method -->
    <div class="hero-body" style="text-align: center;">
      <h2 class="title is-3">Scene Editing with AI Agent</h2>
      <!--      <h2 class="subtitle has-text-centered" style="font-size: 14px;">-->
      <p style="text-align: justify;">
        Given user prompts, our GPT-powered agent interprets editing intent and autonomously performs scene edits via our 4D CLIP feature field. Examples include both <em>geometric</em> (e.g., "extract" and "delete") and <em>appearance</em> (e.g., "change color") editing in 3D space. While results may not be perfect due to imperfect fine-grained feature alignment and non-optimal editing parameter tuning, the agent adaptively refines parameters and applies edits consistently across views and time—greatly reducing the need for manual tuning—and demonstrates robust, interactive 4D scene manipulation.
      </p>
      <br>
      <img
              src="./media/figs/fig6_editing_results.jpg"
              alt="Scene Editing"
              style="margin: 0 auto; max-width: 100%; width: auto; height: auto; display: block;"
      >

    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop" >
    <!--    Method -->
    <div class="hero-body" style="text-align: center;">
      <h2 class="title is-3">VQA with Chatbot Agent</h2>
      <!--      <h2 class="subtitle has-text-centered" style="font-size: 14px;">-->
      <p style="text-align: justify;">
        (Left) Our model supports free-form VQA across diverse question types—<em>general</em>, <em>spatial</em>, and <em>temporal</em>—by distilling InternVideo2 features. (Right) At each timestep, we reconstruct both a 4D radiance field and a 4D feature field, providing more inference sources beyond the input video frame—including local (moving camera) and global (zoomed-out) novel views and their corresponding feature maps—thereby supporting VQA in 4D and enhancing the model's spatiotemporal reasoning capabilities.
      </p>
      <br>
      <img
              src="./media/figs/fig7_VQA.jpg"
              alt="VQA with Chatbot Agent"
              style="margin: 0 auto; max-width: 100%; width: auto; height: auto; display: block;"
      >

    </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{zhou2024feature4x,
      author= {Shijie Zhou and Hui Ren and Yijia Weng and Shuwang Zhang and Zhen Wang and Dejia Xu and Zhiwen Fan and Suya You and Zhangyang Wang and Leonidas Guibas and Achuta Kadambi},
      title = {Feature4X: Bridging Any Monocular Video to 4D Agentic AI with Versatile Gaussian Feature Fields},
      year  = {2024},
    }
</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link" href="" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Website source code based on the Nerfies project page. If you want to reuse their source code, please credit them appropriately.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

<script>
  function toggleSliders() {
    // Get both slider containers
    var slider1 = document.getElementById("slider1-container_teaser");
    var slider2 = document.getElementById("slider2-container_teaser");
    var slider3 = document.getElementById("slider3-container_teaser");

    // Toggle visibility
    // if (slider1.style.display === "none") {
    //   slider1.style.display = "block";
    //   slider2.style.display = "none";
    // } else {
    //   slider1.style.display = "none";
    //   slider2.style.display = "block";
    // }
    if (slider1.style.display === "block") {
      slider1.style.display = "none";
      slider2.style.display = "block";
    } else if (slider2.style.display === "block") {
      slider2.style.display = "none";
      slider3.style.display = "block";
    } else {
      slider3.style.display = "none";
      slider1.style.display = "block";
    }


  }
</script>

<script>
  const promptSelect = document.getElementById('promptSelect');
  // const editButton = document.getElementById('editButton');
  // const resultVideo = document.getElementById('resultVideo');

  // Set the default video path
  // const videoPath = 'data/video.mp4';

  // Mock logic for editing the video based on prompt
  function getResultVideoUrl(prompt) {
    // Simulate result video based on selected prompt
    if (prompt === 'delete_dog') {
      return 'media/supplement_videos/italian_pup/language_editing/geometric_deletion_local.mp4'; // Replace with actual path to the video with the dog deleted
    } else if (prompt === 'change_color') {
      return 'media/supplement_videos/italian_pup/language_editing/appearance_clifford_local.mp4'; // Replace with actual path to the video with dog's color changed
    } else if (prompt === 'extract_dog') {
      return 'media/supplement_videos/italian_pup/language_editing/geometric_extraction_local.mp4'; // Replace with actual path to the video with dog's color changed
    }

    return null;
  }

  // Enable/Disable the edit button based on prompt selection
  promptSelect.addEventListener('change', () => {
    const prompt = promptSelect.value;
    const videoUrl = getResultVideoUrl(prompt);
    if (videoUrl) {
      resultVideo.src = videoUrl;
      resultVideo.style.display = 'block';
    } else {
      alert('Sorry, no matching edit for this prompt.');
    }
  });


</script>

</body>
</html>
