<!--Authors: Hui Ren (rhfeiyang.github.io)-->
<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Feature4X: Bridging Any Monocular Video to 4D Agentic AI with Versatile Gaussian Feature Fields">
  <meta name="keywords" content="Gaussian Splatting, Semantic Understanding, 4D reconstruction">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Feature4X: Bridging Any Monocular Video to 4D Agentic AI with Versatile Gaussian Feature Fields</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="./static/css/components.css">
  <link rel="icon" href="data:,"> <!-- Empty favicon -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="./static/js/site.js"></script>

<!-- slider scripts-->
  <script type="module"
          defer
          src="https://cdn.jsdelivr.net/npm/img-comparison-slider@8/dist/index.js"
  ></script>
  <link
          rel="stylesheet"
          href="https://cdn.jsdelivr.net/npm/img-comparison-slider@8/dist/styles.css"
  />

  <!-- img comparison slider -->
  <!-- https://github.com/sneas/img-comparison-slider -->
</head>
<body>

<!-- 深色模式切换按钮 -->
<button id="theme-toggle" class="theme-toggle" aria-label="切换深色/浅色模式">
  <i class="fas fa-moon"></i>
</button>

<!-- For mobile navigation -->
<!-- 添加阅读进度条 -->
<div class="reading-progress" id="reading-progress"></div>

<!-- 移动端导航触发按钮 -->
<button class="mobile-nav-toggle" id="mobile-nav-toggle" aria-label="Open navigation menu">
  <i class="fas fa-bars"></i>
</button>

<!-- 移动端导航侧边栏 -->
<div class="mobile-nav-sidebar" id="mobile-nav-sidebar">
  <ul>
    <li><a href="#abstract" class="nav-link">Abstract</a></li>
    <li><a href="#overview" class="nav-link">Overview</a></li>
    <li><a href="#sam2" class="nav-link">Segmentation</a></li>
    <li><a href="#sam2-comparison" class="nav-link">SAM2 Comparison</a></li>
    <li><a href="#clip" class="nav-link">Perception</a></li>
    <li><a href="#editing" class="nav-link">Editing</a></li>
    <li><a href="#vqa" class="nav-link">VQA</a></li>
    <li><a href="#bibtex" class="nav-link">BibTeX</a></li>
  </ul>
</div>

<!-- 移动端导航遮罩层 -->
<div class="mobile-nav-overlay" id="mobile-nav-overlay"></div>

<!-- 添加桌面端导航栏 -->
<div class="desktop-nav" id="desktop-nav">
  <div class="nav-container">
    <ul>
      <li><a href="#abstract" class="nav-link">Abstract</a></li>
      <li><a href="#overview" class="nav-link">Overview</a></li>
      <li><a href="#sam2" class="nav-link">Segmentation</a></li>
      <li><a href="#clip" class="nav-link">Perception</a></li>
      <li><a href="#editing" class="nav-link">Editing</a></li>
      <li><a href="#vqa" class="nav-link">VQA</a></li>
      <li><a href="#bibtex" class="nav-link">BibTeX</a></li>
    </ul>
  </div>
</div>







<section class="hero main-header main-header-bg">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Feature4X: Bridging Any Monocular Video to 4D Agentic AI with Versatile Gaussian Feature Fields</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://shijiezhou-ucla.github.io/">Shijie Zhou*</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://rhfeiyang.github.io">Hui Ren*</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://yijiaweng.github.io/">Yijia Weng</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/shuwang-zhang">Shuwang Zhang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://zhenwangwz.github.io/">Zhen Wang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://ir1d.github.io/">Dejia Xu</a><sup>4</sup>,
            </span>
            <span class="author-block">
              <a href="https://zhiwenfan.github.io/">Zhiwen Fan</a><sup>4</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=LkpA-L0AAAAJ&hl=en">Suya You</a><sup>5</sup>,
            </span>
            <span class="author-block">
              <a href="https://vita-group.github.io/">Zhangyang Wang</a><sup>4</sup>,
            </span>
            <span class="author-block">
              <a href="https://profiles.stanford.edu/leonidas-guibas">Leonidas Guibas</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://samueli.ucla.edu/people/achuta-kadambi/">Achuta Kadambi</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors affiliations">
            <span class="author-block"><sup>1</sup>UCLA,</span>
            <span class="author-block"><sup>2</sup>MIT,</span>
            <span class="author-block"><sup>3</sup>Stanford,</span>
            <span class="author-block"><sup>4</sup>UT Austin,</span>
            <span class="author-block"><sup>5</sup>DEVCOM ARL</span>
          </div>
          <p class="is-size-6 cofirst">*Co-first authors</p>
          <div class="flex-center">
            <div class="publication-venue" style="padding: 0.5rem 1.5rem !important; text-align: center !important;">
              <p class="is-size-5">CVPR 2025</p>
            </div>
          </div>
          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="files/Feature4x.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="content-card">
        <div class="video-container">
          <video controls poster="./media/figs/fig1_feature4x.jpg">
            <source src="./media/demo.mp4" type="video/mp4">
            Demo Video
          </video>
        </div>
        <h2 class="subtitle has-text-centered video-caption">
          Feature4X: Building 4D Interactive Scenes with Agentic AI from Monocular Videos. By dynamically distilling modelconditioned features and integrating 2D foundation models with LLMs in feedback loops, Feature4X enables multimodal tasks across 2D, 3D, and 4D with high-level language inputs or direct user interactions, including (but not limited to) segmentation, scene editing, and VQA across novel views and all time steps, unlocking new possibilities for 4D agentic AI.
        </h2>
      </div>
    </div>
  </div>
</section>

<section class="section hero is-light abstract-section abstract-section-padding">
  <span class="section-anchor" id="abstract"></span>
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <div class="has-text-centered">
          <h2 class="title is-3 section-title">
            <span class="section-title-decoration">✦</span>
            Abstract
            <span class="section-title-decoration">✦</span>
          </h2>
        </div>
        <div class="content-card">
          <div class="content has-text-left">
            <p class="text-section">
              Recent advancements in 2D and multimodal models have achieved remarkable success by leveraging large-scale training on extensive datasets. However, extending these achievements to enable free-form interactions and high-level semantic operations with complex 3D/4D scenes remains challenging. This difficulty stems from the limited availability of large-scale, annotated 3D/4D or multi-view datasets, which are crucial for generalizable vision and language tasks such as open-vocabulary and prompt-based segmentation, language-guided editing, and visual question answering (VQA). In this paper, we introduce <em>Feature4X</em>, a universal framework designed to extend any functionality from 2D vision foundation model into the 4D realm, using only monocular video input, which is widely available from user-generated content. The "X" in Feature4X represents its versatility, enabling any task through adaptable, model-conditioned 4D feature field distillation. At the core of our framework is a dynamic optimization strategy that unifies multiple model capabilities into a single representation. Additionally, to the best of our knowledge, Feature4X is the first method to distill and lift the features of video foundation models (e.g. SAM2, InternVideo2) into an explicit <em>4D feature field</em> using Gaussian Splatting. Our experiments showcase novel view segment anything, geometric and appearance scene editing, and free-form VQA across all time steps, empowered by LLMs in feedback loops. These advancements broaden the scope of agentic AI applications by providing a foundation for scalable, contextually and spatiotemporally aware systems capable of immersive dynamic 4D scene interaction.
            </p>
          </div>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!--    &lt;!&ndash; Paper video. &ndash;&gt;-->
    <!--    <div class="columns is-centered has-text-centered">-->
    <!--      <div class="column is-four-fifths">-->
    <!--        <h2 class="title is-3">Video</h2>-->
    <!--        <div class="publication-video">-->
    <!--          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"-->
    <!--                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>-->
    <!--        </div>-->
    <!--      </div>-->
    <!--    </div>-->
    <!--    &lt;!&ndash;/ Paper video. &ndash;&gt;-->
  </div>
</section>

<section class="hero teaser">
  <span class="section-anchor" id="overview"></span>
  <div class="container is-max-desktop" >
    <!--    Method -->
    <div class="hero-body">
      <div class="has-text-centered">
        <h2 class="title is-3 section-title">
          <span class="section-title-decoration">✦</span>
          Overview
          <span class="section-title-decoration">✦</span>
        </h2>
      </div>
      <div class="content-card">
        <p class="text-section">
          Given an input monocular video, we infer 2D priors to segment static background (represented by <em>static 3D Gaussians</em> augmented with latent features) and dynamic foreground (represented by <em>dynamic 3D Gaussians</em> guided by Motion Scaffolds, a set of nodes {v<sub>i</sub>} encoding 3D motion trajectories and latent features h<sub>1</sub>). Dynamic Gaussian features and motions are computed via interpolation from their K-nearest scaffold nodes. At each timestep, dynamic Gaussians are warped and fused with static Gaussians. A parallel rasterization generates RGB images and a unified latent feature map, decoded into task-specific features—illustrated here by SAM2, CLIP-LSeg, and InternVideo2 for representative 2D (novel view segmentation), 3D (scene editing), and 4D (spatiotemporal VQA) tasks. Our framework generalizes to any 2D vision foundation model and is trained end-to-end using input RGB frames and customized features from pretrained 2D models. At inference, rendered feature maps from arbitrary views and timesteps are directly fed into task-specific decoders, seamlessly supporting user prompts and LLM interactions to form a unified 4D agentic AI system.
        </p>
        <div class="img-container">
          <img
                  src="./media/figs/fig2_pipeline.jpg"
                  alt="Method Pipeline"
                  class="standard-image"
          >
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <span class="section-anchor" id="sam2"></span>
  <div class="container is-max-desktop" >
    <!--    Method -->
    <div class="hero-body">
      <div class="has-text-centered">
        <h2 class="title is-3 section-title">
          <span class="section-title-decoration">✦</span>
          Segment Anything in Dynamic 4D Scenes with SAM2 Feature Field
          <span class="section-title-decoration">✦</span>
        </h2>
      </div>
      <div class="content-card">
        <p class="text-section">
          For any rendered novel view video, we support: (a) <em>Promptless segmentation</em> (segment everything): when no user prompt is provided, segmentation masks are automatically assigned at the first frame (t=0) and then propagated across all frames. (b) <em>Promptable segmentation</em> (segment anything): the user can segment any object—static or dynamic—at any timestep using a point or box prompt, and the corresponding mask is robustly tracked and propagated through subsequent frames.
        </p>
        <div class="img-container">
          <img
                  src="./media/figs/fig3_sam.png"
                  alt="Segment Anything"
                  class="standard-image"
          >
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <span class="section-anchor" id="sam2-comparison"></span>
  <div class="container is-max-desktop" >
    <!--    Method -->
    <div class="hero-body">
      <div class="has-text-centered">
        <h2 class="title is-3 section-title">
          <span class="section-title-decoration">✦</span>
          Baseline Comparison on SAM2 Inference
          <span class="section-title-decoration">✦</span>
        </h2>
      </div>
      <div class="content-card">
        <p class="text-section">
        We compare segmentation quality and inference speed between (a) the naive RGB-based approach and (b) our feature-based method. Ours achieves comparable segmentation, accurately tracking the object over time, and avoids RGB artifacts (red box region at t=70), while reducing inference time to about 4× speed-up.
        </p>
        <div class="img-container">
          <img
                  src="./media/figs/fig4_sam_compare.png"
                  alt="Baseline Comparison"
                  class="standard-image"
          >
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <span class="section-anchor" id="clip"></span>
  <div class="container is-max-desktop" >
    <!--    Method -->
    <div class="hero-body">
      <div class="has-text-centered">
        <h2 class="title is-3 section-title">
          <span class="section-title-decoration">✦</span>
          Semantic 4D Scene Understanding with CLIP Feature Field
          <span class="section-title-decoration">✦</span>
        </h2>
      </div>
      <div class="content-card">
        <p class="text-section">
          By lifting CLIP-LSeg features into a 4D feature field, we enable <em>pixel-level semantic segmentation</em> from any view at any timestep. This allows robust 4D scene understanding, even as object appearances change over time—for example, accurately identifying a blooming flower from bud to full bloom across views.
        </p>
        <div class="img-container">
          <img
                  src="./media/figs/fig5_clip_seg_results.jpg"
                  alt="Scene Understanding"
                  class="standard-image"
          >
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <span class="section-anchor" id="editing"></span>
  <div class="container is-max-desktop" >
    <!--    Method -->
    <div class="hero-body">
      <div class="has-text-centered">
        <h2 class="title is-3 section-title">
          <span class="section-title-decoration">✦</span>
          Scene Editing with AI Agent
          <span class="section-title-decoration">✦</span>
        </h2>
      </div>
      <div class="content-card">
        <p class="text-section">
          Given user prompts, our GPT-powered agent interprets editing intent and autonomously performs scene edits via our 4D CLIP feature field. Examples include both <em>geometric</em> (e.g., "extract" and "delete") and <em>appearance</em> (e.g., "change color") editing in 3D space. While results may not be perfect due to imperfect fine-grained feature alignment and non-optimal editing parameter tuning, the agent adaptively refines parameters and applies edits consistently across views and time—greatly reducing the need for manual tuning—and demonstrates robust, interactive 4D scene manipulation.
        </p>
        <div class="img-container">
          <img
                  src="./media/figs/fig6_editing_results.jpg"
                  alt="Scene Editing"
                  class="standard-image"
          >
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <span class="section-anchor" id="vqa"></span>
  <div class="container is-max-desktop" >
    <!--    Method -->
    <div class="hero-body">
      <div class="has-text-centered">
        <h2 class="title is-3 section-title">
          <span class="section-title-decoration">✦</span>
          VQA with Chatbot Agent
          <span class="section-title-decoration">✦</span>
        </h2>
      </div>
      <div class="content-card">
        <p class="text-section">
          (Left) Our model supports free-form VQA across diverse question types—<em>general</em>, <em>spatial</em>, and <em>temporal</em>—by distilling InternVideo2 features. (Right) At each timestep, we reconstruct both a 4D radiance field and a 4D feature field, providing more inference sources beyond the input video frame—including local (moving camera) and global (zoomed-out) novel views and their corresponding feature maps—thereby supporting VQA in 4D and enhancing the model's spatiotemporal reasoning capabilities.
        </p>
        <div class="img-container">
          <img
                  src="./media/figs/fig7_VQA.jpg"
                  alt="VQA with Chatbot Agent"
                  class="standard-image"
          >
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section feature-section feature-section-padding" id="BibTeX">
  <span class="section-anchor" id="bibtex"></span>
  <div class="container is-max-desktop content">
    <div class="content-card bibtex-container">
      <div class="bibtex-header">
        <h3 class="bibtex-title">BibTeX</h3>
        <button id="copy-bibtex" class="copy-button">
          <i class="fas fa-copy"></i> Copy
        </button>
      </div>
      <pre class="bibtex-content"><code id="bibtex-content">@inproceedings{zhou2025feature4x,
  title={Feature4X: Bridging Any Monocular Video to 4D Agentic AI with Versatile Gaussian Feature Fields},
  author={Zhou, Shijie and Ren, Hui and Weng, Yijia and Zhang, Shuwang and Wang, Zhen and Xu, Dejia and Fan, Zhiwen and You, Suya and Wang, Zhangyang and Guibas, Leonidas and Kadambi},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2025}
}</code></pre>
    </div>
  </div>
</section>


<footer class="footer footer-custom">
  <div class="container">
    <div class="content has-text-centered">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="footer-icons">
            <a class="footer-icon-link" href="">
              <i class="fab fa-github"></i>
            </a>
            <a class="footer-icon-link" href="">
              <i class="fas fa-globe"></i>
            </a>
            <a class="footer-icon-link" href="">
              <i class="fas fa-envelope"></i>
            </a>
          </div>
          <div class="footer-content">
            <p class="footer-text">
              This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" class="footer-link">Creative
              Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
            <p class="footer-text-small">
              Website source code based on the Nerfies project page. If you want to reuse their source code, please credit them appropriately.
            </p>
            <p class="footer-text-copyright">
              © 2025 Feature4X Team - All Rights Reserved
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Scroll to top button -->
<button id="scroll-to-top" title="Go to top" class="scroll-top-button">
  <i class="fas fa-chevron-up"></i>
</button>

<!-- 添加图片预览弹出层 -->
<div class="lightbox-overlay" id="lightbox">
  <div class="lightbox-close" id="lightbox-close">
    <i class="fas fa-times"></i>
  </div>
  <div class="lightbox-container">
    <img src="" alt="" class="lightbox-image" id="lightbox-image">
    <div class="lightbox-caption" id="lightbox-caption"></div>
  </div>
</div>

</body>
</html>


