<!--Authors: Hui Ren (rhfeiyang.github.io)-->
<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Feature4X: Bridging Any Monocular Video to 4D Agentic AI with Versatile Gaussian Feature Fields">
  <meta name="keywords" content="Gaussian Splatting, Semantic Understanding, 4D reconstruction">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Feature4X: Bridging Any Monocular Video to 4D Agentic AI with Versatile Gaussian Feature Fields</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="data:,"> <!-- Empty favicon -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

<!-- slider scripts-->
  <script
          defer
          src="https://cdn.jsdelivr.net/npm/img-comparison-slider@8/dist/index.js"
  ></script>
  <link
          rel="stylesheet"
          href="https://cdn.jsdelivr.net/npm/img-comparison-slider@8/dist/styles.css"
  />

  <!-- swiper -->
  <!-- https://swiperjs.com/demos -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/swiper@10/swiper-bundle.min.css"/>
  <script src="https://cdn.jsdelivr.net/npm/swiper@10/swiper-bundle.min.js"></script>


  <!-- img comparison slider -->
  <!-- https://github.com/sneas/img-comparison-slider -->


</head>
<body>

<style>
  .container_teaser {
    display: flex; /* Enables Flexbox */
    justify-content: center; /* Horizontally centers the content */
    align-items: center; /* Vertically centers the content */
    height: 100%; /* Set the height of the container */
  }

  .column_teaser {
    flex: 0 0 auto; /* This ensures that the column doesn't grow or shrink */
    width: 100%; /* Adjust this width as needed */
    max-width: 500px; /* Adjust this max-width as needed */
    margin: 0 auto; /* Centers the column within the container */
  }
</style>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Feature4X: Bridging Any Monocular Video to 4D Agentic AI with Versatile Gaussian Feature Fields</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://shijiezhou-ucla.github.io/">Shijie Zhou*</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://rhfeiyang.github.io">Hui Ren*</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://yijiaweng.github.io/">Yijia Weng</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/shuwang-zhang">Shuwang Zhang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://zhenwangwz.github.io/">Zhen Wang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://ir1d.github.io/">Dejia Xu</a><sup>4</sup>,
            </span>
            <span class="author-block">
              <a href="https://zhiwenfan.github.io/">Zhiwen Fan</a><sup>4</sup>
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=LkpA-L0AAAAJ&hl=en">Suya You</a><sup>5</sup>
            </span>
            <span class="author-block">
              <a href="https://vita-group.github.io/">Zhangyang Wang</a><sup>4</sup>
            </span>
            <span class="author-block">
              <a href="https://profiles.stanford.edu/leonidas-guibas">Leonidas Guibas</a><sup>3</sup>
            </span>
            <span class="author-block">
              <a href="https://samueli.ucla.edu/people/achuta-kadambi/">Achuta Kadambi</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>UCLA,</span>
            <span class="author-block"><sup>2</sup>MIT,</span>
            <span class="author-block"><sup>3</sup>Stanford,</span>
            <span class="author-block"><sup>4</sup>UT-Austin,</span>
            <span class="author-block"><sup>5</sup>DEVCOM ARL</span>
          </div>
          <p class="is-size-6">*Co-first authors</p>
          <p class="is-size-5" style="color: rgb(169, 16, 151); --darkreader-inline-color: var(--darkreader-text-a91097, #ef5ade);">CVPR 2025</p>
          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!--              &lt;!&ndash; Video Link. &ndash;&gt;-->
              <!--              <span class="link-block">-->
              <!--                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"-->
              <!--                   class="external-link button is-normal is-rounded is-dark">-->
              <!--                  <span class="icon">-->
              <!--                      <i class="fab fa-youtube"></i>-->
              <!--                  </span>-->
              <!--                  <span>Video</span>-->
              <!--                </a>-->
              <!--              </span>-->
              <!-- Code Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!--              &lt;!&ndash; Dataset Link. &ndash;&gt;-->
              <!--              <span class="link-block">-->
              <!--                <a href="https://github.com/google/nerfies/releases/tag/0.1"-->
              <!--                   class="external-link button is-normal is-rounded is-dark">-->
              <!--                  <span class="icon">-->
              <!--                      <i class="far fa-images"></i>-->
              <!--                  </span>-->
              <!--                  <span>Data</span>-->
              <!--                  </a>-->
              <!--            </div>-->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video controls poster="./media/teaser/fig1_teaser.jpg" style="margin: 0 auto; max-width: 100%; width: auto; height: auto; display: block;">
        <source src="./media/demo.mp4" type="video/mp4">
        Demo Video
      </video>
      <h2 class="subtitle has-text-centered" style="font-size: 14px;">
        We introduce a universal framework designed to extend any functionality from 2D vision foundation model into 4D realm, using only monocular video input, opening the door to a brand new semantic, editable, and promptable explicit 4D scene representation with agentic AI.
      </h2>
    </div>
  </div>
</section>


<!--<section class="hero teaser">-->
<!--  <div class="container_teaser is-max-desktop">-->
<!--    <div class="column_teaser" style="margin-right: 0;">-->
<!--      &lt;!&ndash; First Slider &ndash;&gt;-->
<!--      <div id="slider1-container_teaser">-->
<!--        <img-comparison-slider id="slider1">-->
<!--          <figure slot="first" class="before">-->
<!--            <img src="./media/teaser/novel_rgb_local1.gif"-->
<!--                 style="margin: 0 auto;max-width: 100%; width: auto; height: auto; display: block;"-->
<!--            >-->
<!--            <figcaption>Radiance Field </figcaption>-->
<!--          </figure>-->
<!--          <figure slot="second" class="after">-->
<!--            <img src="./media/teaser/novel_fmap_clip_local_1.gif"-->
<!--                 style="margin: 0 auto;max-width: 100%; width: auto; height: auto; display: block;"-->
<!--            >-->
<!--            <figcaption>Distilled Feature Field (LSeg)</figcaption>-->
<!--          </figure>-->
<!--        </img-comparison-slider>-->
<!--      </div>-->

<!--      &lt;!&ndash; Second Slider, initially hidden &ndash;&gt;-->
<!--      <div id="slider2-container_teaser" style="display: none;">-->
<!--        <img-comparison-slider id="slider2">-->
<!--          <figure slot="first" class="before">-->
<!--            <img src="./media/teaser/novel_rgb_local2.gif"-->
<!--                 style="margin: 0 auto;max-width: 100%; width: auto; height: auto; display: block;"-->
<!--            >-->
<!--            <figcaption>Radiance Field </figcaption>-->
<!--          </figure>-->
<!--          <figure slot="second" class="after">-->
<!--            <img src="./media/teaser/novel_fmap_sam2_local_1.gif"-->
<!--                 style="margin: 0 auto;max-width: 100%; width: auto; height: auto; display: block;"-->
<!--            >-->
<!--            <figcaption>Distilled Feature Field (SAM)</figcaption>-->
<!--          </figure>-->
<!--        </img-comparison-slider>-->
<!--      </div>-->
<!--      &lt;!&ndash;     Third slider &ndash;&gt;-->
<!--      <div id="slider3-container_teaser" style="display: none;">-->
<!--        <img-comparison-slider id="slider3">-->
<!--          <figure slot="first" class="before">-->
<!--            <img src="./media/teaser/novel_rgb_local3.gif"-->
<!--                 style="margin: 0 auto;max-width: 100%; width: auto; height: auto; display: block;"-->
<!--            >-->
<!--            <figcaption>Radiance Field </figcaption>-->
<!--          </figure>-->
<!--          <figure slot="second" class="after" >-->
<!--            <img src="./media/teaser/novel_fmap_internvideo_local.gif"-->
<!--                 style="margin: 0 auto;max-width: 100%; width: auto; height: auto; display: block;"-->
<!--            >-->
<!--            <figcaption>Distilled Feature Field (Internvideo)</figcaption>-->
<!--          </figure>-->
<!--        </img-comparison-slider>-->
<!--      </div>-->

<!--      <div class="button-container_teaser">-->
<!--        <div class="glow-button" style="display: flex; justify-content: center;">-->
<!--          <button id="toggle-slider" onclick="toggleSliders()">Switch Feature Field 🪄</button>-->
<!--        </div>-->
<!--      </div>-->
<!--&lt;!&ndash;      <h2 class="subtitle has-text-centered">&ndash;&gt;-->
<!--&lt;!&ndash;        Feature 3DGS distills 3D feature fields from any 2D foundation models, opening the door to a brand new semantic, editable, and promptable explicit 3D scene representation.&ndash;&gt;-->
<!--&lt;!&ndash;      </h2>&ndash;&gt;-->
<!--    </div>-->

<!--    <div class="column_teaser" style="margin-left: 0;">-->

<!--      <video id="resultVideo" autoplay muted loop style="max-width: 100%; width: auto; height: auto; display: block;">-->
<!--        <source src="media/supplement_videos/italian_pup/language_editing/appearance_clifford_local.mp4" type="video/mp4" >-->

<!--      </video>-->

<!--      <select id="promptSelect" title="Select a option" style="margin-top: 5px;display: block; margin-left: auto; margin-right: auto;">-->
<!--        &lt;!&ndash;        <option value="" disabled selected>Select an option</option>&ndash;&gt;-->
<!--        <option value="change_color">Make the dog's color look like Clifford</option>-->
<!--        <option value="delete_dog">Delete the dog</option>-->
<!--        <option value="extract_dog">Extract the dog</option>-->
<!--        &lt;!&ndash; Add more options if needed &ndash;&gt;-->
<!--      </select>-->

<!--    </div>-->
<!--  </div>-->
<!--</section>-->



<!--<section class="hero is-light is-small">-->
<!--  <div class="hero-body">-->
<!--    <div class="container">-->
<!--      <div id="results-carousel" class="carousel results-carousel">-->
<!--        <div class="item item-steve">-->
<!--          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">-->
<!--            <source src="./static/videos/steve.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--        </div>-->
<!--        <div class="item item-chair-tp">-->
<!--          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">-->
<!--            <source src="./static/videos/chair-tp.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--        </div>-->
<!--        <div class="item item-shiba">-->
<!--          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">-->
<!--            <source src="./static/videos/shiba.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--        </div>-->
<!--        <div class="item item-fullbody">-->
<!--          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">-->
<!--            <source src="./static/videos/fullbody.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--        </div>-->
<!--        <div class="item item-blueshirt">-->
<!--          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">-->
<!--            <source src="./static/videos/blueshirt.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--        </div>-->
<!--        <div class="item item-mask">-->
<!--          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">-->
<!--            <source src="./static/videos/mask.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--        </div>-->
<!--        <div class="item item-coffee">-->
<!--          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">-->
<!--            <source src="./static/videos/coffee.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--        </div>-->
<!--        <div class="item item-toby">-->
<!--          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">-->
<!--            <source src="./static/videos/toby2.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
<!--        </div>-->
<!--      </div>-->
<!--    </div>-->
<!--  </div>-->
<!--</section>-->


<section class="section hero is-light">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Recent advancements in 2D and multi-modal models have achieved remarkable success by leveraging large-scale training on extensive datasets. However, extending these achievements to enable free-form interactions and high-level semantic operations with complex 3D/4D scenes remains challenging. This difficulty stems from the limited availability of large-scale, annotated 3D/4D or multi-view datasets, which are crucial for generalizable vision and language tasks such as open-vocabulary and prompt-based segmentation, language-guided editing, and visual question answering (VQA). In this paper, we introduce Feature4X, a universal framework designed to extend any functionality from 2D vision foundation model into 4D realm, using only monocular video input, which is widely available from user-generated content. The "X" in Feature4X represents its versatility, enabling any task through adaptable, model-conditioned 4D feature field distillation. At the core of our framework is a dynamic optimization strategy that unifies multiple model capabilities into a single, task-dependent representation. Additionally, to the best of our knowledge, we are the first method to distill and lift the video foundation models (e.g. SAM2, InternVideo2) features into an explicit 4D feature field using Gaussian Splatting. Our experiments showcase novel view segment anything, geometric and appearance scene editing, and free-form VQA across all time steps, empowered by LLMs in feedback loops. These advancements broaden the scope of agentic AI applications by providing a foundation for scalable, contextually and spatiotemporally aware systems capable of immersive dynamic 4D scene interaction.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!--    &lt;!&ndash; Paper video. &ndash;&gt;-->
    <!--    <div class="columns is-centered has-text-centered">-->
    <!--      <div class="column is-four-fifths">-->
    <!--        <h2 class="title is-3">Video</h2>-->
    <!--        <div class="publication-video">-->
    <!--          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"-->
    <!--                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>-->
    <!--        </div>-->
    <!--      </div>-->
    <!--    </div>-->
    <!--    &lt;!&ndash;/ Paper video. &ndash;&gt;-->
  </div>
</section>

<br>
<section class="hero teaser">
  <div class="container is-max-desktop" >
    <!--    Method -->
    <div class="hero-body" style="text-align: center;">
      <h2 class="title is-3">Overview</h2>
      <!--      <h2 class="subtitle has-text-centered" style="font-size: 14px;">-->
      <p style="text-align: justify;">
        We build upon MoSca and infer 2D priors on the input monocular video to separate the scene into static background and dynamic foreground elements. The static background is represented by a set of static 3D Gaussians (GS) with latent features, while the dynamic foreground is modeled with dynamic 3D Gaussians guided by Motion Scaffolds, a set of nodes {<i>v<sub>i</sub></i>} representing 3D motion trajectories over time and base latent features <i>h<sub>i</sub></i>. We compute the motion and latent features of dynamic 3D Gaussians by interpolating their <i>K</i>-Nearest Scaffold nodes. Given a target timestep <i>t</i>, we warp dynamic Gaussians from different timesteps based on their motions, fuse them with static Gaussians, and rasterize their colors and latent features into an RGB image and a <i>D</i>-dimensional feature map, respectively. The feature map is then fed into task-dependent decoders <i>&#120097;<sub>s</sub></i> to support tasks such as 2D semantic segmentation (SAM), 3D editing (CLIP-LSeg), and 4D query-based reasoning (InternVideo2).
      </p>
      <br>
      <img
              src="./media/pipeline/fig2_pipeline_v1.jpg"
              alt="Method Pipeline"
              style="margin: 0 auto; max-width: 100%; width: auto; height: auto; display: block;"
      >

    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop" >
    <!--    Method -->
    <div class="hero-body" style="text-align: center;">
      <h2 class="title is-3">Novel view segmentation (SAM2) results</h2>
      <!--      <h2 class="subtitle has-text-centered" style="font-size: 14px;">-->
      <p style="text-align: justify;">
        Overview of the 4D SAM-based mask assignment and propagation across a dynamic scene. (Left) A 4D scene representation and corresponding 4D SAM feature field. (Right) (a) Mask assignment demonstrates the segmentation of various scene elements, including buildings, roads, and vehicles, at time t=0 (b) Mask propagation across subsequent time frames (from t=10 to t=30 ) illustrates how the assigned masks are consistently tracked and updated as objects move through the scene, enabling accurate temporal correspondence. This approach ensures robust segmentation and tracking in dynamic 4D environments.
      </p>
      <br>
      <img
              src="./media/paper_results/sam_seg_results.jpg"
              alt="Method Pipeline"
              style="margin: 0 auto; max-width: 100%; width: auto; height: auto; display: block;"
      >

    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop" >
    <!--    Method -->
    <div class="hero-body" style="text-align: center;">
      <h2 class="title is-3">Novel view segmentation (CLIP) results</h2>
      <!--      <h2 class="subtitle has-text-centered" style="font-size: 14px;">-->
      <p style="text-align: justify;">
        Visualization of semantic understanding and view synthesis in a 4D scene with a flower pot. (Top left) The 4D scene captures the object and its environment. (Top right) Novel views generated by the model demonstrate realistic perspective shifts of the flower pot. (Bottom left) The 4D CLIP feature field representation encodes semantic information about the scene. (Bottom right) Semantic masks show the segmentation of the flower pot, wall, window, and curtain over time (t=10,55,95), demonstrating consistent semantic labeling and tracking across frames. This approach enables robust 4D scene understanding and segmentation.
      </p>
      <br>
      <img
              src="./media/paper_results/clip_seg_results.jpg"
              alt="Method Pipeline"
              style="margin: 0 auto; max-width: 100%; width: auto; height: auto; display: block;"
      >

    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop" >
    <!--    Method -->
    <div class="hero-body" style="text-align: center;">
      <h2 class="title is-3">Language-guided 4D scene editing results</h2>
      <!--      <h2 class="subtitle has-text-centered" style="font-size: 14px;">-->
      <p style="text-align: justify;">
        Language-guided 4D scene editing results. Demonstration of high-level object-specific manipulations using natural language prompts. (Top Row) One time frame of novel view angle generated by the model. (Bottom Row) Same frame of the entire reconstructed 4D scene. (Left) "Extract the swan," where the swan is isolated from its background. (Center) ”Delete the camel,” results in the removal of the camel from the scene and reconstruction of the background. (Right) ”Change the cow color to black and white” modifies the cow’s color to grayscale.
      </p>
      <br>
      <img
              src="./media/paper_results/editing_results.jpg"
              alt="Method Pipeline"
              style="margin: 0 auto; max-width: 100%; width: auto; height: auto; display: block;"
      >

    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop" >
    <!--    Method -->
    <div class="hero-body" style="text-align: center;">
      <h2 class="title is-3">Novel view synthesis</h2>
      <!--      <h2 class="subtitle has-text-centered" style="font-size: 14px;">-->
      <p style="text-align: justify;">
        Q & A Illustration of local and global novel view synthesis from an input video frame. (Top Left) Original input frame, featuring a dog on a window ledge. (Top Center, Right) Local novel view and its corresponding feature representation, capturing fine details and localized perspective shifts. (Bottom) global novel view and its feature representation, showing the full scene’s spatial context and structure.
      </p>
      <br>
      <img
              src="./media/paper_results/novel_view.jpg"
              alt="Method Pipeline"
              style="margin: 0 auto; max-width: 100%; width: auto; height: auto; display: block;"
      >

    </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{zhou2024feature4x,
      author= {Shijie Zhou and Hui Ren and Yijia Weng and Shuwang Zhang and Zhen Wang and Dejia Xu and Zhiwen Fan and Suya You and Zhangyang Wang and Leonidas Guibas and Achuta Kadambi},
      title = {Feature4X: Bridging Any Monocular Video to 4D Agentic AI with Versatile Gaussian Feature Fields},
      year  = {2024},
    }
</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link" href="" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Website source code based on the Nerfies project page. If you want to reuse their source code, please credit them appropriately.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

<script>
  function toggleSliders() {
    // Get both slider containers
    var slider1 = document.getElementById("slider1-container_teaser");
    var slider2 = document.getElementById("slider2-container_teaser");
    var slider3 = document.getElementById("slider3-container_teaser");

    // Toggle visibility
    // if (slider1.style.display === "none") {
    //   slider1.style.display = "block";
    //   slider2.style.display = "none";
    // } else {
    //   slider1.style.display = "none";
    //   slider2.style.display = "block";
    // }
    if (slider1.style.display === "block") {
      slider1.style.display = "none";
      slider2.style.display = "block";
    } else if (slider2.style.display === "block") {
      slider2.style.display = "none";
      slider3.style.display = "block";
    } else {
      slider3.style.display = "none";
      slider1.style.display = "block";
    }


  }
</script>

<script>
  const promptSelect = document.getElementById('promptSelect');
  // const editButton = document.getElementById('editButton');
  // const resultVideo = document.getElementById('resultVideo');

  // Set the default video path
  // const videoPath = 'data/video.mp4';

  // Mock logic for editing the video based on prompt
  function getResultVideoUrl(prompt) {
    // Simulate result video based on selected prompt
    if (prompt === 'delete_dog') {
      return 'media/supplement_videos/italian_pup/language_editing/geometric_deletion_local.mp4'; // Replace with actual path to the video with the dog deleted
    } else if (prompt === 'change_color') {
      return 'media/supplement_videos/italian_pup/language_editing/appearance_clifford_local.mp4'; // Replace with actual path to the video with dog's color changed
    } else if (prompt === 'extract_dog') {
      return 'media/supplement_videos/italian_pup/language_editing/geometric_extraction_local.mp4'; // Replace with actual path to the video with dog's color changed
    }

    return null;
  }

  // Enable/Disable the edit button based on prompt selection
  promptSelect.addEventListener('change', () => {
    const prompt = promptSelect.value;
    const videoUrl = getResultVideoUrl(prompt);
    if (videoUrl) {
      resultVideo.src = videoUrl;
      resultVideo.style.display = 'block';
    } else {
      alert('Sorry, no matching edit for this prompt.');
    }
  });


</script>

</body>
</html>
